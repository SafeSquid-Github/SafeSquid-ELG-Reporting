input {
  udp {
    port => 10514
    type => "safesquid_logs"
    host => "0.0.0.0"
  }
}

filter {
## Config
  if "conf" in [message] {
    mutate { add_tag => ["conf"] }

    # Extract hostname and log data
    grok {
      match => { "message" => "<%{NUMBER:syslog_pri}>%{SYSLOGTIMESTAMP:syslog_timestamp} %{HOSTNAME:hostname} conf %{GREEDYDATA:log_data}" }
    }

    mutate {
      replace => { "message" => "%{log_data}" }
      remove_field => ["log_data", "syslog_timestamp", "syslog_pri"]
    }

    # Extract SafeSquid configuration details
    grok {
      match => { "message" => "\"%{DATA:access_time}\"\t\"%{DATA:safesquid_interface}\"\t\"%{DATA:username}\"\t\"%{DATA:section}\"\t\"%{DATA:action}\"\t\"%{DATA:arguments}\"\t\"%{DATA:config_file}\"\t\"%{DATA:client_id}\"\t\"%{DATA:page}\"\t\"%{DATA:http_method}\"\t\"%{DATA:url}\"\t\"%{DATA:referer}\"\t\"%{DATA:reason}\"" }
    }

    if "_grokparsefailure" in [tags] { drop { } }

    mutate {
      strip => ["safesquid_interface", "username", "section", "action", "arguments", "config_file", "client_id", "page", "http_method", "url", "referer", "reason", "hostname"]
    }

    # Convert `"-"` to `null` for all fields
    ruby {
      code => '
      event.to_hash.each do |k, v|
        if v.is_a?(String)  # Process only string fields
        if v == "-" or v.strip.empty?
          event.set(k, nil)  # Convert "-" or empty strings to null
        end
        end
      end
      '
    }

    # Convert access_time to @timestamp
    date {
      match => ["access_time", "dd/MMM/yyyy:HH:mm:ss"]
      target => "@timestamp"
      timezone => "Asia/Kolkata"
    }

    # **Extract individual key-value pairs from `arguments`**
    kv {
      source => "arguments"
      field_split => "&"
      value_split => "="
      remove_field => ["arguments"]  # Remove original arguments field after extraction
    }

    # Extract IP from host field (if present)
    if [host][ip] {
      mutate {
        add_field => { "server_ip" => "%{[host][ip]}" }
      }
    }

    mutate {
      remove_field => ["message", "event", "access_time", "host"]
    }
  }

## csp
  if "csp" in [message] {
    mutate {
      add_tag => ["csp"]
    }

    # Extract hostname and JSON data
    grok {
      match => { "message" => "^<\d+>%{SYSLOGTIMESTAMP} %{HOSTNAME:hostname} csp %{GREEDYDATA:json_data}" }
    }

    # Ensure JSON extraction succeeded before replacing message
    if [json_data] {
      mutate {
        replace => { "message" => "%{json_data}" }
        remove_field => ["json_data"]
      }
    } 

    # Remove "csp " prefix before JSON parsing
    mutate {
      gsub => ["message", "^csp ", ""]
    }

    # Parse JSON from the message field
    json {
      source => "message"
    }

    # If JSON parsing succeeds, rename fields
    if "_jsonparsefailure" not in [tags] {
      mutate {
        rename => { "[csp-report][blocked-uri]" => "blocked_uri" }
        rename => { "[csp-report][column-number]" => "column_number" }
        rename => { "[csp-report][disposition]" => "disposition" }
        rename => { "[csp-report][document-uri]" => "document_uri" }
        rename => { "[csp-report][effective-directive]" => "effective_directive" }
        rename => { "[csp-report][original-policy]" => "original_policy" }
        rename => { "[csp-report][referrer]" => "referrer" }
        rename => { "[csp-report][status-code]" => "status_code" }
        rename => { "[csp-report][violated-directive]" => "violated_directive" }
        rename => { "[csp-report][source-file]" => "source-file" }
        rename => { "[csp-report][line-number]" => "line-number" }
        rename => { "[from][user]" => "user" }
        rename => { "[from][date]" => "report_date" }
        rename => { "[info][CLIENTID]" => "client_id" }
        rename => { "[info][USERNAME]" => "username" }
        rename => { "[info][handler]" => "handler" }
      }

      # Convert Unix timestamp to @timestamp
      date {
        match => ["report_date", "UNIX"]
        target => "@timestamp"
        timezone => "Asia/Kolkata"
      }

      # Extract IP from host field (if present)
      if [host][ip] {
        mutate {
          add_field => { "server_ip" => "%{[host][ip]}" }
        }
      }

      # Ensure `host` is removed after extracting `ip` and `hostname`
      mutate {
        remove_field => ["message", "report_date", "event", "host"]
      }
    }
  }  

## performance
  if "performance" in [message] {
    # Ensure only one tag
    mutate {
      remove_field => ["tags"]
      add_tag => ["performance"]
    }

    # Extract syslog components correctly
    grok {
      match => { "message" => "^<\d+>%{SYSLOGTIMESTAMP} %{HOSTNAME:hostname} performance %{GREEDYDATA:log_data}" }
    }

    # Fix hostname field (removes extra spaces)
    mutate {
      strip => ["hostname"]
    }

    # Ensure only necessary data remains
    if [log_data] {
      mutate {
        replace => { "message" => "%{log_data}" }
        remove_field => ["pri", "month", "day", "time", "log_data"]
      }
    }


    # Use dissect to correctly parse CSV fields
    dissect {
      mapping => {
        "message" => "%{timestamp},%{elapsed_time},%{client_connections_handled},%{client_connections_closed},%{client_transactions_handled},%{client_connections_in_pool},%{spare_client_threads},%{client_threads_in_use},%{client_threads_in_waiting},%{threads_starting_up},%{threads_reserved_for_prefetching},%{threading_errors},%{outbound_connections_created},%{outbound_connections_failed},%{outbound_connection_pool_reused},%{outbound_connections_in_pool},%{bytes_in_kb},%{bytes_out_kb},%{caching_objects_created_in_memory},%{caching_objects_removed_from_memory},%{dns_queries_reused},%{new_dns_queries},%{dns_query_failures},%{total_system_memory_kb},%{free_system_memory_kb},%{safesquid_virtual_memory_kb},%{safesquid_resident_memory_kb},%{safesquid_shared_memory_kb},%{safesquid_code_memory_kb},%{safesquid_data_memory_kb},%{safesquid_library_memory_kb},%{connections_handled_delta},%{connections_closed_delta},%{transactions_handled_delta},%{client_pool_delta},%{spare_threads_delta},%{active_threads_delta},%{threads_waiting_delta},%{threads_starting_up_delta},%{threads_prefetching_delta},%{threading_errors_delta},%{outbound_connections_created_delta},%{outbound_connections_failed_delta},%{outbound_connection_pool_reused_delta},%{outbound_connections_in_pool_delta},%{bytes_in_kb_delta},%{bytes_out_kb_delta},%{caching_objects_created_in_memory_delta},%{caching_objects_removed_from_memory_delta},%{dns_queries_reused_delta},%{new_dns_queries_delta},%{dns_query_failures_delta},%{load_avg_1min},%{load_avg_5min},%{load_avg_15min},%{running_processes},%{waiting_processes},%{user_time},%{system_time},%{total_time},%{user_time_delta},%{system_time_delta},%{total_time_delta}"
      }
    }

    # Convert timestamp to @timestamp
    date {
      match => ["timestamp", "yyyyMMddHHmmss"]
      target => "@timestamp"
      timezone => "Asia/Kolkata"
    }

    # Extract IP from host field (if present)
    if [host][ip] {
      mutate {
        add_field => { "server_ip" => "%{[host][ip]}" }
      }
    }

    # Ensure correct field cleanup
    mutate {
      remove_field => ["timestamp", "message", "event", "log", "host"]
    }
  }  

## extended
  if "extended" in [message] {
    mutate {
      add_tag => ["extended"]
    }

    # Extract hostname from syslog header
    grok {
      match => { "message" => "^<\d+>%{SYSLOGTIMESTAMP} %{HOSTNAME:hostname} extended %{GREEDYDATA:log_data}" }
    }

    # Ensure we keep only necessary data
    if [log_data] {
      mutate {
        replace => { "message" => "%{log_data}" }
        remove_field => ["log_data"]
      }
    }

    # Parse structured fields
    grok {
      match => { "message" => "\"%{DATA:record_id}\"\t\"%{DATA:client_id}\"\t\"%{DATA:request_id}\"\t\"%{DATA:date_time}\"\t\"%{DATA:elapsed_time}\"\t\"%{DATA:status}\"\t\"%{DATA:size}\"\t\"%{DATA:upload}\"\t\"%{DATA:download}\"\t\"%{DATA:bypassed}\"\t\"%{DATA:client_ip}\"\t\"%{DATA:username}\"\t\"%{DATA:method}\"\t\"%{DATA:url}\"\t\"%{DATA:http_referer}\"\t\"%{DATA:useragent}\"\t\"%{DATA:mime}\"\t\"%{DATA:filter_name}\"\t\"%{DATA:filtering_reason}\"\t\"%{DATA:interface}\"\t\"%{DATA:cachecode}\"\t\"%{DATA:peercode}\"\t\"%{DATA:peer}\"\t\"%{DATA:request_host}\"\t\"%{DATA:request_tld}\"\t\"%{DATA:referer_host}\"\t\"%{DATA:referer_tld}\"\t\"%{DATA:range}\"\t\"%{DATA:time_profiles}\"\t\"%{DATA:user_groups}\"\t\"%{DATA:request_profiles}\"\t\"%{DATA:application_signatures}\"\t\"%{DATA:categories}\"\t\"%{DATA:response_profiles}\"\t\"%{DATA:upload_content_types}\"\t\"%{DATA:download_content_types}\"\t\"%{DATA:profiles}\"" }
    }

    date {
      match => [ "date_time", "dd/MMM/yyyy:HH:mm:ss" ]
      target => "@timestamp"  # Convert date_time to @timestamp
      timezone => "Asia/Kolkata"  # Stores in IST
    }

    # Extract IP from host field
    if [host][ip] {
      mutate {
        add_field => { "server_ip" => "%{[host][ip]}" }
      }
    }

    # Remove unwanted fields after extracting IP and hostname
    mutate {
      remove_field => ["date_time", "event", "host", "message"]
    }
  }
}

output {
  # Store Configuration Logs
  if "conf" in [tags] {
    elasticsearch {
      hosts => ["http://localhost:9200"]
      index => "safesquid-conf_%{+YYYY_MM}"
      template => "/etc/logstash/templates/safesquid_conf_template.json"
      template_name => "safesquid_conf_template"
      template_overwrite => true
      manage_template => true
    }
  }

  # Store CSP Logs
  if "csp" in [tags] {
    elasticsearch {
      hosts => ["http://localhost:9200"]
      index => "safesquid-csp_%{+YYYY_MM}"
      template => "/etc/logstash/templates/safesquid_csp_template.json"
      template_name => "safesquid_csp_template"
      template_overwrite => true
      manage_template => true
    }
  }

  # Store Extended Logs
  if "extended" in [tags] {
    elasticsearch {
      hosts => ["http://localhost:9200"]
      index => "safesquid-ext_%{+YYYY_MM}"
      template => "/etc/logstash/templates/safesquid_ext_template.json"
      template_name => "safesquid_ext_template"
      template_overwrite => true
      manage_template => true
    }
  }

  # Store Performance Logs
  if "performance" in [tags] {
    elasticsearch {
      hosts => ["http://localhost:9200"]
      index => "safesquid-perf_%{+YYYY_MM}"
      template => "/etc/logstash/templates/safesquid_perf_template.json"
      template_name => "safesquid_perf_template"
      template_overwrite => true
      manage_template => true
    }
  }

  # Debugging Output
  # stdout { codec => rubydebug }
}
